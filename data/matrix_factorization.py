# -*- coding: utf-8 -*-
"""Matrix Factorization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QWHOrhsOHnKmxl5y2fPKhQPzP6y59cav

# Necessary Imports
"""

import pandas as pd
import numpy as np

from sklearn.utils import shuffle
from sklearn.decomposition import NMF

"""### Reading data Files"""

data = pd.read_csv('data_train.csv')

#Column rename to get better understanding
data.rename(columns={'row':'user','col':'item','data':'rating'}, inplace=True)
data.head()

from collections import defaultdict

user = defaultdict(list)
item = defaultdict(list)

for i in range(len(data)):
  user[data.iloc[i,0]].append(data.iloc[i,1])
  item[data.iloc[i,1]].append(data.iloc[i,0])

"""### Reading Content Matrix File"""

feature_data = pd.read_csv('data_ICM_title_abstract.csv')

#Raname columns for better understanding
feature_data.rename(columns = {'row':'item','col':'feature','data':'value'}, inplace= True)
feature_data.head()

"""### Creating Matrix for content as well as interaction"""

#matrix that stores interaction as values item as rows and users as column
interaction_mat = np.zeros((max(data['item'].values)+1, max(data['user'].values)+1))         

for i in range(len(data)):
    interaction_mat[data.iloc[i,1]][data.iloc[i,0]] = data.iloc[i, 2]
    
    

#matrix that stores feature as values items as rows and feature ids as column    
content_mat = np.zeros((max(feature_data['item'].values)+1,max(feature_data['feature'].values)+1))

for i in range(len(feature_data)):
    content_mat[feature_data.iloc[i, 0]][feature_data.iloc[i, 1]] = feature_data.iloc[i, 2]

"""## Embedding Model for User and Item ( will used for recommendation )"""

#Here you can pass any matrix item content or user item..... I passed content matrix over Here.


model = NMF(n_components = 8, init='random', random_state = 42)
W = model.fit_transform(interaction_mat) #row wala item means items
H = model.components_                #column wala item means features/users

print(W.shape)

"""### Utility Functions to get Recommnedation as well as Similar Objects"""

from sklearn.metrics.pairwise import cosine_similarity

# sorting function to return top n values
def b_sort(arr, n):
    count = 0
    l = len(arr)
    while count < n:
        for i in range(l-1, 0, -1):
            if arr[i] > arr[i-1]:
                arr[i], arr[i-1] = arr[i-1], arr[i]
        count += 1
        
    return arr[:n]
            

# function to return similar objects based on cosine similarity of embeddings

def top_similar_objects(element_id, n, identifier, skip=1):

    #Checking for object to be item or user
    if identifier == 'item':
        predictor = W[element_id]
        embedding = W
    else:
        predictor = H.T[element_id]
        embedding = H.T
        
        
    r = np.array([predictor])                                #pedicting embeddings using custome utilty model
    
    results = cosine_similarity(r, embedding)[0] # calculating cosine similarity
    
    #map utility to get ID of objects
    map = defaultdict(list)
    for i in range(len(results)):
        map[results[i]].append(i)

        
    ans = []
    score = []
    results = sorted(results, reverse = True) #sorting results
    count = 0
    finish = False
    for i in range(skip, n+skip):
      for item in map[results[i]]:
        ans.append(item)
        score.append(results[i])
        count += 1
        if count == 10:
          finish = True
          break
      if finish:
        break
        #ans.append([map[results[len(results)-1-i]],results[len(results)-1-i]]) with score
        
    return ans, score, map


def recommendation(user_id, K, N=10):
    map = defaultdict()
    for value in data[data['user'] == user_id]['item'].values:
        #print(value)
        a, s, m = top_similar_objects(value, N, 'item')
        map.update(m)
        
    '''
    for i in range(len(score)):
        map[score[i]] = ans[i]
    '''

    map = dict(sorted(map.items(), reverse=True))
    
    count = 0
    results = []
    finish = False
    
    for item, values in map.items():
        for value in set(values):
            results.append(value)
            count += 1
            if count == K:
                finish = True
                break
        if finish:
            break

    return results

"""### Testing Utilty Functions"""

object_type = 'item'    #Change Value Here to test
top_k = 10              #Change Value Here to test
object_id = 5       #Change Value Here to test

ans, score, m = top_similar_objects(object_id, top_k, object_type, 0)

for item, sc in zip(ans, score):
    print('Similar Object ID: ',item,'Score: ',sc)

"""1447 637 5085 7639 13219 9851 8544 24911 13797 6782

20477 18264 3178 23866 14593 21908 7116 21232 6755 12742

12061 5717 23127 25491 773 5119 13837 13363 5906 2724
"""

print(recommendation(0, 10))

"""### Reading data for target users"""

testing = pd.read_csv('data_target_users_test.csv')
testing.head()

results = []
count = 0
for item in testing.loc[:,'user_id'].values:
    r = recommendation(item, 10)
    results.append(r)
    count += 1
    print(count)

#testing.to_csv('results.csv')
with open('test.csv', 'w') as f:
  f.write('user_id,item_list')

with open('test.csv', 'a') as f:
  for i in range(0, len(results)):
    f.write('\n'+','.join([str(i), ' '.join(list(map(str, results[i])))]))

"""###  Below this is a implementation of Matrix factorisation from scratch Need Not Run """

class matrixFactorization():
    
    def __init__(self, data, features):
        
        self.data = data
        self.features = features
        self.user_count = data.shape[0]
        self.item_count = data.shape[1]
        self.user_features = np.random.uniform(low = 0.1, high = 0.9, size = (self.item_count, self.features))
        self.item_features = np.random.uniform(low = 0.1, high = 0.9, size = (self.features, self.item_count))
        
        
        
    def MSE(self):
        
        matrix_product = np.matmul(self.user_features, self.item_features)
        return np.sum((self.data - matrix_product)**2)
    
    
    
    
    def single_gradient(self, user_row, item_col, wrt_user_idx= None, wrt_item_idx= None):
        
        if wrt_user_idx != None and wrt_item_idx != None:
            
            return 'Too many elements'
        
        elif wrt_user_idx == None and wrt_item_idx == None:
            
            return 'Insufficient elements'
        
        else:
            u_row = self.user_features[user_row, :]
            i_col = self.item_features[:, item_col]
            ui_rating = float(self.data[user_row, item_col])
            prediction = float(np.dot(u_row, i_col))
            
            if wrt_user_idx != None:
                row_elem = float(i_col[wrt_user_idx])
                gradient = 2*(ui_rating - prediction)*row_elem
            else:
                col_elem = float(u_row[wrt_item_idx])
                gradient = 2*(ui_rating - prediction)*col_elem
                
            return gradient
        
        
        
        
    def user_feature_gradient(self, user_row, wrt_user_idx):

        summation = 0

        for col in range(self.item_count):
            summation += self.single_gradient(user_row = user_row, item_col = col, wrt_user_idx= wrt_user_idx)

        return summation/self.item_count






    def item_feature_gradient(self, item_col, wrt_item_idx):

        summation = 0

        for row in range(self.user_count):
            summation += self.single_gradient(user_row = row, item_col = item_col, wrt_item_idx= wrt_item_idx)

        return summation/self.user_count







    def update_user_feature(self, learning_rate):

        for i in range(self.user_count):
            for j in range(self.features):
                self.user_features[i, j] += learning_rate*self.user_feature_gradient(user_row= i, wrt_user_idx=j)





    def update_item_feature(self, learning_rate):

        for i in range(self.features):
            for j in range(self.item_count):
                self.item_features[i, j] += learning_rate*self.item_feature_gradient(item_col = j, wrt_item_idx=i)




    def train_model(self, learning_rate=0.1, iterations=1000):


        for i in range(iterations):
            self.update_user_feature(learning_rate = learning_rate)
            self.update_item_feature(larning_rate = learning_rate)
            if i%50 == 0:
                print(self.MSE())

"""### Analysis of file

### Data Preparation for Model

### Training the Model

### Utilty Models to get Embedding for a given user or item

### Verification Of similar object function

### Function to get trending Items
"""

#This function utility will return top trending items
#Note It can be update with any logic You desire.
def trending(n):
  return [0]

"""### Top K recommendation function

### Reading target file

### Getting Reults for target values

## Run To write a test.csv
"""